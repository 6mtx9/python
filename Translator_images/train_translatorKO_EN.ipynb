{"cells":[{"cell_type":"code","execution_count":37,"id":"bea831a2","metadata":{"id":"bea831a2","executionInfo":{"status":"ok","timestamp":1692224396661,"user_tz":-120,"elapsed":3,"user":{"displayName":"mtx","userId":"04588200501890111357"}}},"outputs":[],"source":["from transformers import pipeline\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n","from datasets import load_dataset_builder\n","from datasets import load_dataset\n","import numpy as np\n","import evaluate\n","import torch"]},{"cell_type":"code","source":[],"metadata":{"id":"D_iPR5Asb0cz"},"id":"D_iPR5Asb0cz","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":38,"id":"631c094a","metadata":{"id":"631c094a","executionInfo":{"status":"ok","timestamp":1692224426899,"user_tz":-120,"elapsed":17728,"user":{"displayName":"mtx","userId":"04588200501890111357"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5a7f4916-1641-456a-d4ce-ab9538eed268"},"outputs":[{"output_type":"stream","name":"stderr","text":["Generate config GenerationConfig {\n","  \"_from_model_config\": true,\n","  \"decoder_start_token_id\": 0,\n","  \"eos_token_id\": 1,\n","  \"pad_token_id\": 0,\n","  \"transformers_version\": \"4.31.0\"\n","}\n","\n","All model checkpoint weights were used when initializing LongT5ForConditionalGeneration.\n","\n","All the weights of LongT5ForConditionalGeneration were initialized from the model checkpoint at KETI-AIR-Downstream/long-ke-t5-base-translation-aihub-ko2en.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use LongT5ForConditionalGeneration for predictions without further training.\n","Generation config file not found, using a generation config created from the model config.\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(\"KETI-AIR-Downstream/long-ke-t5-base-translation-aihub-ko2en\",model_max_length=128)\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"KETI-AIR-Downstream/long-ke-t5-base-translation-aihub-ko2en\")"]},{"cell_type":"code","execution_count":null,"id":"5257141c","metadata":{"id":"5257141c"},"outputs":[],"source":["def prepare_dataset(data):\n","    source_language = [value['ko'] for key, value in data.items()]\n","    target_language = [value['en'] for key, value in data.items()]\n","    return source_language, target_language"]},{"cell_type":"code","execution_count":null,"id":"cf77cc76","metadata":{"id":"cf77cc76"},"outputs":[],"source":["train  = load_dataset(\"Moo/korean-parallel-corpora\", split=\"train\")\n","#test = load_dataset(\"Moo/korean-parallel-corpora\", split=\"test\")\n","validation = load_dataset(\"Moo/korean-parallel-corpora\", split=\"validation\")"]},{"cell_type":"code","execution_count":null,"id":"a8d4031c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a8d4031c","executionInfo":{"status":"ok","timestamp":1692214019597,"user_tz":-120,"elapsed":12,"user":{"displayName":"mtx","userId":"04588200501890111357"}},"outputId":"4bfad984-75a2-4369-e4bd-4e3c3529a564"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'ko': '개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"',\n"," 'en': 'Much of personal computing is about \"can you top this?\"'}"]},"metadata":{},"execution_count":5}],"source":["train[0]"]},{"cell_type":"code","execution_count":null,"id":"4b75620f","metadata":{"id":"4b75620f"},"outputs":[],"source":["inputs = tokenizer(train['ko'],return_tensors=\"pt\", max_length=128, truncation=True,padding=True)\n","outputs = tokenizer(train['en'],return_tensors=\"pt\",max_length=128, truncation=True,padding=True)"]},{"cell_type":"code","source":["inputs2 = tokenizer(validation['ko'],return_tensors=\"pt\", max_length=128, truncation=True,padding=True)\n","outputs2 = tokenizer(validation['en'],return_tensors=\"pt\",max_length=128, truncation=True,padding=True)"],"metadata":{"id":"q0KfjLo_GOtY"},"id":"q0KfjLo_GOtY","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"9e8d9131","metadata":{"id":"9e8d9131"},"outputs":[],"source":["dataset = torch.utils.data.TensorDataset(inputs.input_ids, inputs.attention_mask, outputs.input_ids, outputs.attention_mask)"]},{"cell_type":"code","source":["dataset2 = torch.utils.data.TensorDataset(inputs2.input_ids, inputs2.attention_mask, outputs2.input_ids, outputs2.attention_mask)"],"metadata":{"id":"sDKLqeU-GSAm"},"id":"sDKLqeU-GSAm","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"ac540a90","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ac540a90","executionInfo":{"status":"ok","timestamp":1692214059548,"user_tz":-120,"elapsed":13,"user":{"displayName":"mtx","userId":"04588200501890111357"}},"outputId":"e54eccb6-089b-407a-b1f0-6b1d8aedaea4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([  381, 11023,   832,    54,     5,  1310,   202,    12, 20004, 20023,\n","          6704,   121,  2265,    33,  1739,    19, 20016,  1577, 20787,     1,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0]),\n"," tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0]),\n"," tensor([25907, 20010, 20536, 27426, 20017, 20069, 20004, 20023, 22623, 20025,\n","         20357, 20039, 20787,     1,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0]),\n"," tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"]},"metadata":{},"execution_count":8}],"source":["dataset[0]"]},{"cell_type":"code","execution_count":null,"id":"d66690c5","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"d66690c5","executionInfo":{"status":"ok","timestamp":1692223224440,"user_tz":-120,"elapsed":1376778,"user":{"displayName":"mtx","userId":"04588200501890111357"}},"outputId":"529ea695-6fe3-48b6-fa54-ab12f234107a"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 96,215\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6,014\n","  Number of trainable parameters = 296,696,448\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:835: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4174' max='6014' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4174/6014 50:31 < 22:16, 1.38 it/s, Epoch 0.69/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Saving model checkpoint to ./results/checkpoint-500\n","Configuration saved in ./results/checkpoint-500/config.json\n","Configuration saved in ./results/checkpoint-500/generation_config.json\n","Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-5000] due to args.save_total_limit\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:835: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n","Saving model checkpoint to ./results/checkpoint-1000\n","Configuration saved in ./results/checkpoint-1000/config.json\n","Configuration saved in ./results/checkpoint-1000/generation_config.json\n","Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-5500] due to args.save_total_limit\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:835: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n","Saving model checkpoint to ./results/checkpoint-1500\n","Configuration saved in ./results/checkpoint-1500/config.json\n","Configuration saved in ./results/checkpoint-1500/generation_config.json\n","Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-1500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-6000] due to args.save_total_limit\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:835: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n","Saving model checkpoint to ./results/checkpoint-2000\n","Configuration saved in ./results/checkpoint-2000/config.json\n","Configuration saved in ./results/checkpoint-2000/generation_config.json\n","Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-2000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-500] due to args.save_total_limit\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:835: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n","Saving model checkpoint to ./results/checkpoint-2500\n","Configuration saved in ./results/checkpoint-2500/config.json\n","Configuration saved in ./results/checkpoint-2500/generation_config.json\n","Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-2500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-2500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-1000] due to args.save_total_limit\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:835: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n","Saving model checkpoint to ./results/checkpoint-3000\n","Configuration saved in ./results/checkpoint-3000/config.json\n","Configuration saved in ./results/checkpoint-3000/generation_config.json\n","Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-3000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-1500] due to args.save_total_limit\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:835: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n","Saving model checkpoint to ./results/checkpoint-3500\n","Configuration saved in ./results/checkpoint-3500/config.json\n","Configuration saved in ./results/checkpoint-3500/generation_config.json\n","Model weights saved in ./results/checkpoint-3500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-3500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-3500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-2000] due to args.save_total_limit\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:835: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n","Saving model checkpoint to ./results/checkpoint-4000\n","Configuration saved in ./results/checkpoint-4000/config.json\n","Configuration saved in ./results/checkpoint-4000/generation_config.json\n","Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-4000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-4000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-2500] due to args.save_total_limit\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:835: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='6014' max='6014' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6014/6014 1:13:16, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.646200</td>\n","      <td>0.811003</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-4500\n","Configuration saved in ./results/checkpoint-4500/config.json\n","Configuration saved in ./results/checkpoint-4500/generation_config.json\n","Model weights saved in ./results/checkpoint-4500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-4500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-4500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-3000] due to args.save_total_limit\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:835: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n","Saving model checkpoint to ./results/checkpoint-5000\n","Configuration saved in ./results/checkpoint-5000/config.json\n","Configuration saved in ./results/checkpoint-5000/generation_config.json\n","Model weights saved in ./results/checkpoint-5000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-5000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-5000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-3500] due to args.save_total_limit\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:835: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n","Saving model checkpoint to ./results/checkpoint-5500\n","Configuration saved in ./results/checkpoint-5500/config.json\n","Configuration saved in ./results/checkpoint-5500/generation_config.json\n","Model weights saved in ./results/checkpoint-5500/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-5500/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-5500/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-4000] due to args.save_total_limit\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:835: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n","Saving model checkpoint to ./results/checkpoint-6000\n","Configuration saved in ./results/checkpoint-6000/config.json\n","Configuration saved in ./results/checkpoint-6000/generation_config.json\n","Model weights saved in ./results/checkpoint-6000/pytorch_model.bin\n","tokenizer config file saved in ./results/checkpoint-6000/tokenizer_config.json\n","Special tokens file saved in ./results/checkpoint-6000/special_tokens_map.json\n","Deleting older checkpoint [results/checkpoint-4500] due to args.save_total_limit\n","/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:835: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n","***** Running Evaluation *****\n","  Num examples = 1000\n","  Batch size = 16\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Saving model checkpoint to ./train_translatorKO_EN\n","Configuration saved in ./train_translatorKO_EN/config.json\n","Configuration saved in ./train_translatorKO_EN/generation_config.json\n","Model weights saved in ./train_translatorKO_EN/pytorch_model.bin\n","tokenizer config file saved in ./train_translatorKO_EN/tokenizer_config.json\n","Special tokens file saved in ./train_translatorKO_EN/special_tokens_map.json\n"]}],"source":["from transformers import TrainingArguments, Trainer\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"./results\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    weight_decay=0.01,\n","    save_total_limit=3,\n","    num_train_epochs=1,\n","    fp16=True,\n","    remove_unused_columns=False,\n","    logging_dir=\"./logs\",\n",")\n","\n","def data_collator(batch):\n","        return {\n","            \"input_ids\": torch.stack([item[0] for item in batch]),\n","            \"attention_mask\": torch.stack([item[1] for item in batch]),\n","            \"labels\": torch.stack([item[2] for item in batch]),\n","        }\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dataset,\n","    eval_dataset=dataset2,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")\n","\n","\n","import transformers\n","transformers.logging.set_verbosity_info()\n","\n","trainer.train()\n","\n","# Save the trained model\n","output_dir = \"./train_translatorKO_EN\"\n","trainer.save_model(output_dir)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"nvQT8drtbwaj"},"id":"nvQT8drtbwaj","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls \"/content/drive/My Drive\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N8MMTkzJYhjf","executionInfo":{"status":"ok","timestamp":1692224448083,"user_tz":-120,"elapsed":391,"user":{"displayName":"mtx","userId":"04588200501890111357"}},"outputId":"f4bb211d-5d7b-4a84-be93-fcdab663b7db"},"id":"N8MMTkzJYhjf","execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":[" 1950972215_3-V2_2023-06_Document_Inscription_20221114.pdf\n","'ADHERENT(1).sql'\n","'AP2 ALGO.zip'\n"," Book.gslides\n"," Book.pptx\n","'cahierdescharges_ap2 (1).gdoc'\n"," cahierdescharges_ap2.gdoc\n"," ColabNotebooks\n"," cours\n","'Depense 2022 - 2023.gsheet'\n"," E4\n"," eclipse-workspace\n"," léonie\n","'Mod�le attestation de stage - SIO.gdoc'\n"," OPEN\n","'php array.php'\n"," poursuite-etudes-inge.pdf\n"," poursuite-etudes-pas-inge.pdf\n"," Releve_de_Notes_195097221500002.pdf\n"," TD4.zip\n"," UTILISATEUR.sql\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"MrPFOk1XbRSg","executionInfo":{"status":"ok","timestamp":1692224313320,"user_tz":-120,"elapsed":373,"user":{"displayName":"mtx","userId":"04588200501890111357"}}},"id":"MrPFOk1XbRSg","execution_count":33,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"xjaG0fHSYvDF"},"id":"xjaG0fHSYvDF","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.10(tensorflow)","language":"python","name":"tf"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}